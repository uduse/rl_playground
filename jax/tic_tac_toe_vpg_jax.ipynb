{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:51.702845Z",
     "start_time": "2021-05-02T01:25:50.604135Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "\n",
    "import pyspiel\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad, jit, vmap\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "\n",
    "import optax\n",
    "import haiku as hk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(hk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:52.969537Z",
     "start_time": "2021-05-02T01:25:52.899036Z"
    }
   },
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('tic_tac_toe')\n",
    "gamma = 0.99\n",
    "key = jax.random.PRNGKey(0)\n",
    "dim_board = game.observation_tensor_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:53.696186Z",
     "start_time": "2021-05-02T01:25:53.692983Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_to_repr(state, player):\n",
    "    board_obs = np.asarray(state.observation_tensor(0))\n",
    "    player_obs = np.asarray(player).reshape((1,))\n",
    "    return np.concatenate((board_obs, player_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:54.091168Z",
     "start_time": "2021-05-02T01:25:54.086415Z"
    }
   },
   "outputs": [],
   "source": [
    "all_actions = np.arange(game.num_distinct_actions())\n",
    "def choose_action(state, probs):\n",
    "    probs = np.asarray(probs, dtype=np.float32)\n",
    "    legal_mask = np.asarray(state.legal_actions_mask())\n",
    "    legal_probs = probs * legal_mask\n",
    "    legal_probs /= np.sum(legal_probs)\n",
    "    return np.random.choice(all_actions, p=legal_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:54.540821Z",
     "start_time": "2021-05-02T01:25:54.526165Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self._capacity = capacity\n",
    "        self._data = []\n",
    "        self._next_entry_index = 0\n",
    "\n",
    "    def add(self, element):\n",
    "        if len(self._data) < self._capacity:\n",
    "            self._data.append(element)\n",
    "        else:\n",
    "            self._data[self._next_entry_index] = element\n",
    "            self._next_entry_index += 1\n",
    "            self._next_entry_index %= self._capacity\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        if len(self._data) < num_samples:\n",
    "            raise ValueError(\"{} elements could not be sampled from size {}\".format(\n",
    "              num_samples, len(self._data)))\n",
    "        return random.sample(self._data, num_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:51:07.073264Z",
     "start_time": "2021-05-02T01:51:07.068843Z"
    }
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience', \n",
    "    ['s', 'a', 'r', 's_next', 'v_next_mask']\n",
    ")\n",
    "\n",
    "def f(self):\n",
    "    print('r', self.r)\n",
    "    print(obs_tensor_to_board(self.s), '\\n')\n",
    "    print(obs_tensor_to_board(self.s_next), '\\n')\n",
    "Experience.print = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:55.107707Z",
     "start_time": "2021-05-02T01:25:55.104800Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return jnp.mean(jnp.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:55.639644Z",
     "start_time": "2021-05-02T01:25:55.525142Z"
    }
   },
   "outputs": [],
   "source": [
    "key, value_net_params_init_key = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:56.286361Z",
     "start_time": "2021-05-02T01:25:56.281987Z"
    }
   },
   "outputs": [],
   "source": [
    "# def value_net_hk(x):\n",
    "#     mlp = hk.Sequential([\n",
    "# #         hk.Linear(128), jax.nn.relu,\n",
    "#         hk.Linear(256), jax.nn.relu,\n",
    "#         hk.Linear(128), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(1), jnp.tanh\n",
    "#     ])\n",
    "#     return mlp(x)\n",
    "\n",
    "def value_net_hk(x):\n",
    "    mlp = hk.nets.MLP([64, 64, 1])\n",
    "    return jnp.tanh(mlp(x))\n",
    "\n",
    "value_net = hk.without_apply_rng(hk.transform(value_net_hk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:56.784918Z",
     "start_time": "2021-05-02T01:25:56.690358Z"
    }
   },
   "outputs": [],
   "source": [
    "print(hk.experimental.tabulate(value_net)(jnp.zeros((12, 28))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:57.542807Z",
     "start_time": "2021-05-02T01:25:57.526309Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:25:58.116914Z",
     "start_time": "2021-05-02T01:25:58.108037Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_to_exp(state):\n",
    "    player = state.current_player()\n",
    "    state_repr = state_to_repr(state, player)\n",
    "\n",
    "    action = random.choice(state.legal_actions())\n",
    "    state.apply_action(action)\n",
    "\n",
    "    reward = (state.rewards()[0] + 1) / 2\n",
    "\n",
    "    if state.is_terminal():\n",
    "        next_state_repr = state_to_repr(state, player)\n",
    "        v_next_mask = 0\n",
    "        reward = state.rewards()[0]\n",
    "    else:\n",
    "        next_state_repr = state_to_repr(state, state.current_player())\n",
    "        v_next_mask = 1\n",
    "        reward = 0\n",
    "    exp = Experience(state_repr, action, reward, next_state_repr, v_next_mask)\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:54:06.951391Z",
     "start_time": "2021-05-02T01:54:06.946367Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_batch(samples):\n",
    "    batch = {\n",
    "        's': np.stack([sample.s for sample in samples]),\n",
    "        'a': np.stack([sample.a for sample in samples]),\n",
    "        'r': np.stack([sample.r for sample in samples]),\n",
    "        's_next': np.stack([sample.s_next for sample in samples]),\n",
    "        'v_next_mask': np.stack([sample.v_next_mask for sample in samples]),\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:54:10.299956Z",
     "start_time": "2021-05-02T01:54:10.296687Z"
    }
   },
   "outputs": [],
   "source": [
    "def obs_tensor_to_board(s):\n",
    "    obs = s[:27].reshape(3, 9).T\n",
    "    obs = obs.argmax(axis=-1).reshape(3, 3).squeeze()\n",
    "    return '\\n'.join(map(str, obs)).replace('[', '').replace(']', '') \\\n",
    "                .replace('2', 'x').replace('1', 'o').replace('0', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simi-gradient TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:54:45.929123Z",
     "start_time": "2021-05-02T01:54:45.903922Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def value_net_forward(value_params, x):\n",
    "    v = value_net.apply(value_params, x)\n",
    "    return v\n",
    "\n",
    "@jit\n",
    "def compute_v_next(value_params, batch):\n",
    "    v_next = value_net_forward(value_params, batch['s_next']) * batch['v_next_mask']\n",
    "    return v_next\n",
    "\n",
    "@jit\n",
    "def compute_target(batch):\n",
    "    target = batch['r'] + gamma * batch['v_next']\n",
    "    return target\n",
    "\n",
    "@jit\n",
    "def td_error(value_params, batch):\n",
    "    v = value_net_forward(value_params, batch['s'])\n",
    "    target = compute_target(batch)\n",
    "    error = target - v\n",
    "    return error\n",
    "\n",
    "def value_net_loss(value_params, batch):\n",
    "    error = td_error(value_params, batch)\n",
    "    return jnp.mean(jnp.square(error))\n",
    "    \n",
    "@jit\n",
    "def value_update(value_params, value_opt_state, batch):\n",
    "    loss, grads = value_and_grad(value_net_loss)(value_params, batch)\n",
    "    updates, value_opt_state = value_opt.update(grads, value_opt_state)\n",
    "    new_params = optax.apply_updates(value_params, updates)\n",
    "    return loss, new_params, value_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:33:40.125847Z",
     "start_time": "2021-05-02T01:31:35.703323Z"
    }
   },
   "outputs": [],
   "source": [
    "num_games = 10000\n",
    "buffer = ReplayBuffer(capacity=5000)\n",
    "batch_size = 1000\n",
    "epochs = 100\n",
    "value_net_shape = (batch_size, 28)\n",
    "value_params_old = None\n",
    "value_params = value_net.init(value_net_params_init_key, jnp.zeros(value_net_shape))\n",
    "value_opt = optax.adam(1e-5)\n",
    "value_opt_state = value_opt.init(value_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:36:00.264504Z",
     "start_time": "2021-05-02T01:33:54.421864Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_games)):\n",
    "    state = game.new_initial_state()\n",
    "    while not state.is_terminal():\n",
    "        exp = state_to_exp(state)\n",
    "        buffer.add(exp)\n",
    "\n",
    "    if len(buffer) >= batch_size:\n",
    "        samples = buffer.sample(batch_size)\n",
    "        batch = create_batch(samples)\n",
    "        batch['v_next'] = compute_v_next(value_params_old, batch)\n",
    "        loss, value_params, value_opt_state = value_update(\n",
    "            value_params, value_opt_state, batch)\n",
    "        \n",
    "#     value_params_old = copy.deepcopy(value_params)\n",
    "    value_params_old = value_params\n",
    "    if i % 100 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:54:02.297699Z",
     "start_time": "2021-05-02T01:54:02.280976Z"
    }
   },
   "outputs": [],
   "source": [
    "def traj_to_exps():\n",
    "    state = game.new_initial_state()\n",
    "    obses = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while not state.is_terminal():\n",
    "        obses.append(np.array(state.observation_tensor()))\n",
    "        action = random.choice(state.legal_actions())\n",
    "        actions.append(action)\n",
    "        state.apply_action(action)\n",
    "        if state.is_terminal():\n",
    "            reward = state.rewards()[0]\n",
    "        else:\n",
    "            reward = 0\n",
    "        rewards.append(reward)\n",
    "    obses.append(np.array(state.observation_tensor(0)))\n",
    "    exps = []\n",
    "    for i in range(len(actions)):\n",
    "        action = actions[i]\n",
    "        obs = obses[i]\n",
    "        obs_next = obses[i + 1]\n",
    "        rest_rewards = rewards[i:]\n",
    "        G = 0\n",
    "        for i, r in enumerate(rest_rewards):\n",
    "            G += (gamma ** i) * r\n",
    "        exp = Experience(obs, action, G, obs_next, 0)\n",
    "        exps.append(exp)\n",
    "    for exp in exps:\n",
    "        exp.print()\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def value_net_forward(value_params, x):\n",
    "    v = value_net.apply(value_params, x)\n",
    "    return v\n",
    "\n",
    "@jit\n",
    "def compute_v_next(value_params, batch):\n",
    "    v_next = value_net_forward(value_params, batch['s_next']) * batch['v_next_mask']\n",
    "    return v_next\n",
    "\n",
    "@jit\n",
    "def compute_target(batch):\n",
    "    target = batch['r'] + gamma * batch['v_next']\n",
    "    return target\n",
    "\n",
    "@jit\n",
    "def td_error(value_params, batch):\n",
    "    v = value_net_forward(value_params, batch['s'])\n",
    "    target = compute_target(batch)\n",
    "    error = target - v\n",
    "    return error\n",
    "\n",
    "def value_net_loss(value_params, batch):\n",
    "    error = td_error(value_params, batch)\n",
    "    return jnp.mean(jnp.square(error))\n",
    "    \n",
    "@jit\n",
    "def value_update(value_params, value_opt_state, batch):\n",
    "    loss, grads = value_and_grad(value_net_loss)(value_params, batch)\n",
    "    updates, value_opt_state = value_opt.update(grads, value_opt_state)\n",
    "    new_params = optax.apply_updates(value_params, updates)\n",
    "    return loss, new_params, value_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:54:28.997272Z",
     "start_time": "2021-05-02T01:54:28.972497Z"
    }
   },
   "outputs": [],
   "source": [
    "num_games = 1000\n",
    "buffer = ReplayBuffer(capacity=5000)\n",
    "batch_size = 1000\n",
    "epochs = 100\n",
    "value_net_shape = (batch_size, 28)\n",
    "value_params = value_net.init(value_net_params_init_key, jnp.zeros(value_net_shape))\n",
    "value_opt = optax.adam(1e-5)\n",
    "value_opt_state = value_opt.init(value_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:13:58.635143Z",
     "start_time": "2021-05-02T01:13:43.650019Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_games)):\n",
    "    exps = traj_to_exps()\n",
    "\n",
    "    if len(buffer) >= batch_size:\n",
    "        samples = buffer.sample(batch_size)\n",
    "        batch = create_batch(samples)\n",
    "        batch['v_next'] = compute_v_next(value_params_old, batch)\n",
    "        loss, value_params, value_opt_state = value_update(\n",
    "            value_params, value_opt_state, batch)\n",
    "        \n",
    "# #     value_params_old = copy.deepcopy(value_params)\n",
    "#     value_params_old = value_params\n",
    "#     if i % 100 == 0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T01:48:19.392253Z",
     "start_time": "2021-05-02T01:48:19.310794Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0\n",
      "v [0.45561114]\n",
      "v_next [[0.44146043]]\n",
      "target [[0.43704584]]\n",
      "loss 0.00034467026\n",
      "x . o\n",
      "o o x\n",
      "x . . \n",
      "\n",
      "x . o\n",
      "o o x\n",
      "x x .\n",
      "\n",
      "\n",
      "\n",
      "r -1.0\n",
      "v [0.46478873]\n",
      "v_next [[0.]]\n",
      "target [[-1.]]\n",
      "loss 2.1456058\n",
      "x o x\n",
      ". x x\n",
      "o o . \n",
      "\n",
      "x o x\n",
      ". x x\n",
      "o o o\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.42403108]\n",
      "v_next [[0.46620712]]\n",
      "target [[0.46154505]]\n",
      "loss 0.001407298\n",
      ". . o\n",
      "o . x\n",
      ". x x \n",
      "\n",
      ". o o\n",
      "o . x\n",
      ". x x\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.39510557]\n",
      "v_next [[0.40234292]]\n",
      "target [[0.39831948]]\n",
      "loss 1.0329232e-05\n",
      "x . .\n",
      ". x o\n",
      ". . o \n",
      "\n",
      "x x .\n",
      ". x o\n",
      ". . o\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.43726972]\n",
      "v_next [[0.42403108]]\n",
      "target [[0.41979077]]\n",
      "loss 0.00030551344\n",
      ". . o\n",
      "o . .\n",
      ". x x \n",
      "\n",
      ". . o\n",
      "o . x\n",
      ". x x\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.37565485]\n",
      "v_next [[0.41991568]]\n",
      "target [[0.41571653]]\n",
      "loss 0.0016049384\n",
      ". x .\n",
      "o . .\n",
      ". . . \n",
      "\n",
      ". x .\n",
      "o x .\n",
      ". . .\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.34631684]\n",
      "v_next [[0.37933838]]\n",
      "target [[0.375545]]\n",
      "loss 0.0008542848\n",
      ". . .\n",
      "x . o\n",
      ". x . \n",
      "\n",
      ". . .\n",
      "x . o\n",
      ". x o\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.3733909]\n",
      "v_next [[0.4123707]]\n",
      "target [[0.408247]]\n",
      "loss 0.0012149464\n",
      ". . x\n",
      ". . .\n",
      ". . o \n",
      "\n",
      ". . x\n",
      ". . x\n",
      ". . o\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.37727344]\n",
      "v_next [[0.3758107]]\n",
      "target [[0.3720526]]\n",
      "loss 2.725707e-05\n",
      "o . .\n",
      ". . x\n",
      ". . . \n",
      "\n",
      "o . .\n",
      "x . x\n",
      ". . .\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.41711405]\n",
      "v_next [[0.44818735]]\n",
      "target [[0.44370547]]\n",
      "loss 0.00070710364\n",
      "x . o\n",
      ". o x\n",
      ". . . \n",
      "\n",
      "x . o\n",
      ". o x\n",
      ". . x\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.35925826]\n",
      "v_next [[0.3729418]]\n",
      "target [[0.3692124]]\n",
      "loss 9.9084595e-05\n",
      ". . .\n",
      ". . .\n",
      ". . . \n",
      "\n",
      ". . .\n",
      ". . x\n",
      ". . .\n",
      "\n",
      "\n",
      "\n",
      "r -1.0\n",
      "v [0.3928749]\n",
      "v_next [[0.]]\n",
      "target [[-1.]]\n",
      "loss 1.9401007\n",
      "x x .\n",
      "o o .\n",
      ". x . \n",
      "\n",
      "x x .\n",
      "o o o\n",
      ". x .\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.36906245]\n",
      "v_next [[0.3839616]]\n",
      "target [[0.38012198]]\n",
      "loss 0.00012231304\n",
      "o . .\n",
      "x . o\n",
      "x . x \n",
      "\n",
      "o o .\n",
      "x . o\n",
      "x . x\n",
      "\n",
      "\n",
      "\n",
      "r 1.0\n",
      "v [0.4588078]\n",
      "v_next [[0.]]\n",
      "target [[1.]]\n",
      "loss 0.29288897\n",
      ". . o\n",
      ". x x\n",
      "o o x \n",
      "\n",
      ". . o\n",
      "x x x\n",
      "o o x\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.4522722]\n",
      "v_next [[0.48856544]]\n",
      "target [[0.4836798]]\n",
      "loss 0.000986437\n",
      "o x x\n",
      ". x x\n",
      ". o o \n",
      "\n",
      "o x x\n",
      "o x x\n",
      ". o o\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.45616412]\n",
      "v_next [[0.4584351]]\n",
      "target [[0.45385075]]\n",
      "loss 5.351706e-06\n",
      ". o .\n",
      "o . x\n",
      ". x . \n",
      "\n",
      ". o .\n",
      "o . x\n",
      "x x .\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.37141234]\n",
      "v_next [[0.34810278]]\n",
      "target [[0.34462175]]\n",
      "loss 0.0007177357\n",
      ". . .\n",
      ". o .\n",
      "x x . \n",
      "\n",
      ". . o\n",
      ". o .\n",
      "x x .\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.36613184]\n",
      "v_next [[0.40432802]]\n",
      "target [[0.40028474]]\n",
      "loss 0.0011664203\n",
      ". o .\n",
      ". . .\n",
      "x . x \n",
      "\n",
      ". o .\n",
      ". o .\n",
      "x . x\n",
      "\n",
      "\n",
      "\n",
      "r 1.0\n",
      "v [0.45056763]\n",
      "v_next [[0.]]\n",
      "target [[1.]]\n",
      "loss 0.30187595\n",
      ". o .\n",
      "o x o\n",
      "x x . \n",
      "\n",
      ". o .\n",
      "o x o\n",
      "x x x\n",
      "\n",
      "\n",
      "\n",
      "r 0\n",
      "v [0.3818411]\n",
      "v_next [[0.40675062]]\n",
      "target [[0.4026831]]\n",
      "loss 0.00043438963\n",
      "x . o\n",
      ". . .\n",
      ". . . \n",
      "\n",
      "x . o\n",
      ". x .\n",
      ". . .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp in buffer.sample(20):\n",
    "    if exp:\n",
    "        batch = create_batch([exp])\n",
    "        print('r', exp.r)\n",
    "        print('v', value_net_forward(value_params, jnp.array(exp.s)))\n",
    "        v_next = compute_v_next(value_params, batch)\n",
    "        batch['v_next'] = v_next\n",
    "        print('v_next', v_next)\n",
    "        print('target', compute_target(batch))\n",
    "        print('loss', value_net_loss(value_params, batch))\n",
    "        print(obs_tensor_to_board(exp.s), '\\n')\n",
    "        print(obs_tensor_to_board(exp.s_next))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T23:18:48.510762Z",
     "start_time": "2021-04-30T23:18:48.486479Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "for i in range(10):\n",
    "    print(state)\n",
    "    if not state.is_terminal():\n",
    "        print(state.observation_tensor())\n",
    "    print()\n",
    "    exp = state_to_exp(state)\n",
    "    print(state)\n",
    "    if not state.is_terminal():\n",
    "        print(state.observation_tensor())\n",
    "    print()\n",
    "    print(exp)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T17:42:29.408634Z",
     "start_time": "2021-04-30T17:42:29.398043Z"
    }
   },
   "outputs": [],
   "source": [
    "for sample in buffer.sample(10):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T17:42:21.637798Z",
     "start_time": "2021-04-30T17:42:21.468703Z"
    }
   },
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "while True:\n",
    "    val = value_net.apply(value_params, state_to_repr(state, state.current_player()))\n",
    "    print(state, val, '\\n')\n",
    "    if state.is_terminal():\n",
    "        break\n",
    "    state.apply_action(random.choice(state.legal_actions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_playground_jax",
   "language": "python",
   "name": "rl_playground_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
