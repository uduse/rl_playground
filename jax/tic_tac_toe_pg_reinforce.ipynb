{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T04:14:21.839681Z",
     "start_time": "2021-05-03T04:14:20.737402Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad, jit, vmap\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "import optax\n",
    "import haiku as hk\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pyspiel\n",
    "import numpy as np\n",
    "import trueskill\n",
    "\n",
    "print(hk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T05:06:49.474481Z",
     "start_time": "2021-05-03T05:06:49.468942Z"
    }
   },
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('tic_tac_toe')\n",
    "gamma = 0.99\n",
    "key = jax.random.PRNGKey(0)\n",
    "dim_board = game.observation_tensor_size()\n",
    "dim_actions = game.num_distinct_actions()\n",
    "all_actions = list(range(dim_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T05:06:49.725022Z",
     "start_time": "2021-05-03T05:06:49.709436Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self._capacity = capacity\n",
    "        self._data = []\n",
    "        self._next_entry_index = 0\n",
    "\n",
    "    def add(self, element):\n",
    "        if len(self._data) < self._capacity:\n",
    "            self._data.append(element)\n",
    "        else:\n",
    "            self._data[self._next_entry_index] = element\n",
    "            self._next_entry_index += 1\n",
    "            self._next_entry_index %= self._capacity\n",
    "    \n",
    "    def extend(self, elements):\n",
    "        for ele in elements:\n",
    "            self.add(ele)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        if len(self._data) < num_samples:\n",
    "            raise ValueError(\"{} elements could not be sampled from size {}\".format(\n",
    "              num_samples, len(self._data)))\n",
    "        return random.sample(self._data, num_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T05:06:49.940887Z",
     "start_time": "2021-05-03T05:06:49.934856Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience(object):\n",
    "    s: np.array\n",
    "    a: int\n",
    "    g: float\n",
    "    a_mask: np.array\n",
    "\n",
    "    def print(self):\n",
    "        print('r', self.g)\n",
    "        print(obs_tensor_to_board(self.s), '\\n')\n",
    "        print(obs_tensor_to_board(self.s_next), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T06:25:35.784616Z",
     "start_time": "2021-05-03T06:25:35.780421Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_net_hk(x):\n",
    "    mlp = hk.nets.MLP([128, 64, 32, dim_actions])\n",
    "    return mlp(x)\n",
    "#     return jnp.clip(mlp(x), a_min=50, a_max=51)\n",
    "#     return jnp.tanh(mlp(x))\n",
    "\n",
    "policy_net = hk.without_apply_rng(hk.transform(policy_net_hk))\n",
    "# print(hk.experimental.tabulate(policy_net)(jnp.zeros((12, dim_board))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T06:25:36.059216Z",
     "start_time": "2021-05-03T06:25:36.054307Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_batch(samples):\n",
    "    batch = {\n",
    "        's': np.stack([sample.s for sample in samples]),\n",
    "        'a': np.stack([sample.a for sample in samples]),\n",
    "        'g': np.stack([sample.g for sample in samples]),\n",
    "#         's_next': np.stack([sample.s_next for sample in samples]),\n",
    "        'a_mask': np.stack([sample.a_mask for sample in samples]),\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T06:25:36.339131Z",
     "start_time": "2021-05-03T06:25:36.335155Z"
    }
   },
   "outputs": [],
   "source": [
    "def obs_tensor_to_board(s):\n",
    "    obs = s.reshape(3, 9).T\n",
    "    obs = obs.argmax(axis=-1).reshape(3, 3).squeeze()\n",
    "    return '\\n'.join(map(str, obs)).replace('[', '').replace(']', '') \\\n",
    "                .replace('2', 'x').replace('1', 'o').replace('0', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "@jit\n",
    "def l2_squared(pytree):\n",
    "    leaves, _ = tree_flatten(pytree)\n",
    "    return sum(jnp.vdot(x, x) for x in leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:35.735343Z",
     "start_time": "2021-05-03T16:37:35.703288Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def policy_forward(params, batch):\n",
    "    logits = policy_net.apply(params, batch['s'])\n",
    "    action_probs = jax.nn.softmax(logits)\n",
    "    legal_action_probs = action_probs * batch['a_mask']\n",
    "    legal_action_probs /= jnp.sum(legal_action_probs)\n",
    "    return legal_action_probs\n",
    "\n",
    "@jit\n",
    "def illegal_actions_loss(params, batch):\n",
    "    logits = policy_net.apply(params, batch['s'])\n",
    "    action_probs = jax.nn.softmax(logits)\n",
    "    illegal_mask = jnp.logical_not(batch['a_mask'])\n",
    "    illegal_probs = action_probs * illegal_mask\n",
    "    return jnp.mean(illegal_probs)\n",
    "\n",
    "# @jit\n",
    "# def policy_forward(params, batch):\n",
    "#     logits = policy_net.apply(params, batch['s'])\n",
    "# #     action_probs = jax.nn.softmax(logits - jnp.max(logits, axis=-1, keepdims=True))\n",
    "# #     action_probs += 0.0000001  # epsilon\n",
    "#     action_probs = jax.nn.softmax(logits)\n",
    "# #     action_probs = jnp.clip(action_probs, a_min=0.05)  # epsilon\n",
    "#     legal_action_probs = action_probs * batch['a_mask']\n",
    "#     legal_action_probs /= jnp.sum(legal_action_probs)\n",
    "#     return legal_action_probs\n",
    "\n",
    "@jit\n",
    "def policy_loss(params, batch):\n",
    "    action_probs = policy_forward(params, batch)\n",
    "    log_probs = jnp.log(action_probs)\n",
    "    action = batch['a']\n",
    "    g = batch['g']\n",
    "    selected_log_probs = action_probs[jnp.arange(action.size), action]\n",
    "    ia_loss = illegal_actions_loss(params, batch)\n",
    "#     print(ia_loss)\n",
    "    return -jnp.mean(g * selected_log_probs) + ia_loss\n",
    "\n",
    "@jit\n",
    "def policy_update(params, opt_state, batch):\n",
    "    loss, grads = value_and_grad(policy_loss)(params, batch)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return loss, new_params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:35.898294Z",
     "start_time": "2021-05-03T16:37:35.880359Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpBuilder(object):\n",
    "    def __init__(self):\n",
    "        self.obses = []\n",
    "        self.actions = []\n",
    "        self.action_masks = []\n",
    "        self.reward = None\n",
    "        \n",
    "    def add(self, obs, action, action_mask):\n",
    "        self.actions.append(action)\n",
    "        self.obses.append(obs)\n",
    "        self.action_masks.append(action_mask)\n",
    "        \n",
    "    def set_reward(self, reward):\n",
    "        self.reward = reward\n",
    "#         self.reward = max(self.reward, 0)\n",
    "            \n",
    "    def to_exps(self):\n",
    "        exps = []\n",
    "        for i in range(len(self.actions)):\n",
    "            action = self.actions[i]\n",
    "            action_mask = self.action_masks[i]\n",
    "            obs = self.obses[i]\n",
    "            G = gamma ** (len(self.actions) - 1) * self.reward\n",
    "            exp = Experience(obs, action, G, a_mask=action_mask)\n",
    "            exps.append(exp)\n",
    "        return exps\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obses = []\n",
    "        self.actions = []\n",
    "        self.action_masks = []\n",
    "        self.reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:36.203761Z",
     "start_time": "2021-05-03T16:37:36.140794Z"
    }
   },
   "outputs": [],
   "source": [
    "class Player(object):\n",
    "    def __init__(self):\n",
    "        self.rating = trueskill.Rating()\n",
    "        \n",
    "    def step(self, state):\n",
    "        pass\n",
    "    \n",
    "class RandomPlayer(Player):\n",
    "    def step(self, state):\n",
    "        if not state.is_terminal():\n",
    "            actions = state.legal_actions()\n",
    "            action = random.choice(actions)\n",
    "            return action\n",
    "    \n",
    "class PolicyGradientPlayer(Player):\n",
    "    def __init__(self, player_id, key):\n",
    "        super().__init__()\n",
    "        self.player_id = player_id\n",
    "        self.params = policy_net.init(key, jnp.zeros((1, dim_board)))\n",
    "#         self.params_behave = copy.deepcopy(self.params)\n",
    "        self.opt_state = opt.init(self.params)\n",
    "\n",
    "        self.batch_size = 50\n",
    "        self.builder = ExpBuilder()\n",
    "        self.buffer = ReplayBuffer(1000)\n",
    "        self.last_loss = None\n",
    "        \n",
    "    def get_action_probs(self, state):\n",
    "        obs = np.array(state.observation_tensor(self.player_id))\n",
    "        a_mask = np.array(state.legal_actions_mask())\n",
    "        batch = {'s': obs, 'a_mask': a_mask}\n",
    "        action_probs = np.array(policy_forward(self.params, batch).block_until_ready())\n",
    "        return action_probs\n",
    "    \n",
    "    def get_action_logits(self, state):\n",
    "        obs = np.array(state.observation_tensor(self.player_id))\n",
    "        a_mask = np.array(state.legal_actions_mask())\n",
    "        batch = {'s': obs, 'a_mask': a_mask}\n",
    "        logits = policy_net.apply(self.params, batch['s'])\n",
    "        return logits\n",
    "        \n",
    "    def step(self, state):\n",
    "        self.learn()\n",
    "        \n",
    "        if state.is_terminal():\n",
    "            self.builder.set_reward(state.rewards()[self.player_id])\n",
    "            exps = self.builder.to_exps()\n",
    "            self.buffer.extend(exps)\n",
    "            self.builder.reset()\n",
    "        else:\n",
    "            obs = np.array(state.observation_tensor(self.player_id))\n",
    "            a_mask = np.array(state.legal_actions_mask())\n",
    "            batch = {'s': obs, 'a_mask': a_mask}\n",
    "            action_probs = np.array(policy_forward(self.params, batch).block_until_ready())\n",
    "            action = np.random.choice(all_actions, 1, p=action_probs)\n",
    "            self.builder.add(obs, action, a_mask)\n",
    "            return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "#             for _ in range(20):\n",
    "            samples = self.buffer.sample(self.batch_size)\n",
    "            batch = create_batch(samples)\n",
    "            self.last_loss, self.params, self.opt_state = policy_update(\n",
    "                self.params, self.opt_state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:36.568696Z",
     "start_time": "2021-05-03T16:37:36.565823Z"
    }
   },
   "outputs": [],
   "source": [
    "# networks\n",
    "# key, new_key = jax.random.split(key)\n",
    "# params = policy_net.init(new_key, jnp.zeros((1, dim_board)))\n",
    "opt = optax.adam(1e-5)\n",
    "# opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:36.987651Z",
     "start_time": "2021-05-03T16:37:36.985045Z"
    }
   },
   "outputs": [],
   "source": [
    "key, new_key, new_key_2 = jax.random.split(key, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:37.369641Z",
     "start_time": "2021-05-03T16:37:37.346253Z"
    }
   },
   "outputs": [],
   "source": [
    "agents = [\n",
    "    RandomPlayer(),\n",
    "    PolicyGradientPlayer(1, new_key)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:37.668257Z",
     "start_time": "2021-05-03T16:37:37.666027Z"
    }
   },
   "outputs": [],
   "source": [
    "# agents = [\n",
    "#     PolicyGradientPlayer(0, new_key),\n",
    "#     RandomPlayer(),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:38.081848Z",
     "start_time": "2021-05-03T16:37:38.079418Z"
    }
   },
   "outputs": [],
   "source": [
    "# agents = [\n",
    "#     PolicyGradientPlayer(0, new_key),\n",
    "#     PolicyGradientPlayer(1, new_key_2),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:37:56.927008Z",
     "start_time": "2021-05-03T16:37:38.881502Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_games = 50000\n",
    "wins = {0: 0, 1: 0, 'drawn': 0}\n",
    "for i in tqdm(range(num_games)):\n",
    "    state = game.new_initial_state()\n",
    "    while not state.is_terminal():\n",
    "        player_id = state.current_player()\n",
    "        agent = agents[player_id]\n",
    "        action = agent.step(state)\n",
    "        state.apply_action(action)\n",
    "    for agent in agents:\n",
    "        agent.step(state)\n",
    "        \n",
    "        \n",
    "    drawn = False\n",
    "    if state.rewards()[0] > state.rewards()[1]:\n",
    "        winner, loser = 0, 1\n",
    "        wins[winner] += 1\n",
    "    elif state.rewards()[0] < state.rewards()[1]:\n",
    "        winner, loser = 1, 0\n",
    "        wins[winner] += 1\n",
    "    else:\n",
    "        winner, loser = 0, 1\n",
    "        drawn = True\n",
    "        wins['drawn'] += 1\n",
    "    \n",
    "    agents[winner].rating, agents[loser].rating = \\\n",
    "        trueskill.rate_1vs1(agents[winner].rating, agents[loser].rating, drawn=drawn)\n",
    "    \n",
    "    \n",
    "    if i > 1 and i % 500 == 0:\n",
    "        print(agents[0].rating.mu, agents[1].rating.mu)\n",
    "#         print(agents[0].last_loss)\n",
    "        print(wins)\n",
    "        print()\n",
    "        for agent in agents:\n",
    "            agent.rating = trueskill.Rating()\n",
    "        wins = {0: 0, 1: 0, 'drawn': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T03:39:51.496645Z",
     "start_time": "2021-05-03T03:39:51.490011Z"
    }
   },
   "outputs": [],
   "source": [
    "# logits = policy_net.apply(self.params, batch['s'])\n",
    "# action_probs = jax.nn.softmax(logits - jnp.max(logits, axis=-1, keepdims=True))\n",
    "# legal_action_probs = action_probs * batch['a_mask']\n",
    "# legal_action_probs = legal_action_probs / jnp.sum(legal_action_probs)\n",
    "# return legal_action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:36:04.300292Z",
     "start_time": "2021-05-03T16:36:04.095781Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_games = 20\n",
    "for i in tqdm(range(num_games)):\n",
    "    state = game.new_initial_state()\n",
    "    while not state.is_terminal():\n",
    "        print(state, '\\n')\n",
    "        player_id = state.current_player()\n",
    "        agent = agents[player_id]\n",
    "        print(player_id)\n",
    "        if isinstance(agent, PolicyGradientPlayer):\n",
    "            print(np.round(agent.get_action_probs(state).reshape(3, 3), 2))\n",
    "        action = agent.step(state)\n",
    "#         if player_id == 0:\n",
    "#             print(np.round(agent.get_action_probs(state).reshape(3, 3), 2))\n",
    "#         print(np.round(agent.get_action_probs(state).reshape(3, 3), 2))\n",
    "        state.apply_action(action)\n",
    "    print(state, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_to_exps():\n",
    "    state = game.new_initial_state()\n",
    "    obses = []\n",
    "    actions = []\n",
    "    action_masks = []\n",
    "    rewards = []\n",
    "    while not state.is_terminal():\n",
    "        obses.append(np.array(state.observation_tensor()))\n",
    "        action = random.choice(state.legal_actions())\n",
    "        action_mask = np.array(state.legal_actions_mask())\n",
    "        actions.append(action)\n",
    "        action_masks.append(action_mask)\n",
    "        state.apply_action(action)\n",
    "        if state.is_terminal():\n",
    "            reward = state.rewards()[0]\n",
    "        else:\n",
    "            reward = 0\n",
    "        rewards.append(reward)\n",
    "    obses.append(np.array(state.observation_tensor(0)))\n",
    "    exps = []\n",
    "    for i in range(len(actions)):\n",
    "        action = actions[i]\n",
    "        action_mask = action_masks[i]\n",
    "        obs = obses[i]\n",
    "        obs_next = obses[i + 1]\n",
    "        rest_rewards = rewards[i:]\n",
    "        G = 0\n",
    "        for i, r in enumerate(rest_rewards):\n",
    "            G += (gamma ** i) * r\n",
    "        exp = Experience(obs, action, G, a_mask=action_mask)\n",
    "        exps.append(exp)\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T05:13:20.965142Z",
     "start_time": "2021-05-02T05:13:20.098331Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_games)):\n",
    "    state = game.new_initial_state()\n",
    "    while not state.is_terminal():\n",
    "#         print(state)\n",
    "        if state.current_player() == 0:\n",
    "            a_mask = np.array(state.legal_actions_mask())\n",
    "            batch = {\n",
    "                's': state_to_repr(state),\n",
    "                'a_mask': a_mask\n",
    "            }\n",
    "#             print(a_mask)\n",
    "#             print(action_probs)\n",
    "            action_probs = np.array(policy_forward(params, batch).block_until_ready())\n",
    "#             action_probs[-1] += 1 - np.sum(action_probs)\n",
    "            action = np.random.choice(all_actions, 1, p=action_probs)\n",
    "#             print(action)\n",
    "        else:\n",
    "            action = random.choice(state.legal_actions())\n",
    "        state.apply_action(action)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T02:10:16.643366Z",
     "start_time": "2021-05-03T02:10:16.213956Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_games)):\n",
    "    exps = traj_to_exps()\n",
    "    for exp in exps:\n",
    "        buffer.add(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T02:10:13.662503Z",
     "start_time": "2021-05-03T02:10:13.649871Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch = create_batch(buffer.sample(batch_size))\n",
    "    loss, params, opt_state = policy_update(params, opt_state, batch)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:43:26.487343Z",
     "start_time": "2021-05-02T04:43:26.207736Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exp in buffer.sample(20):\n",
    "    if exp:\n",
    "        batch = create_batch([exp])\n",
    "#         print('g', exp.g)\n",
    "        print('loss', policy_loss(params, batch))\n",
    "        print(obs_tensor_to_board(exp.s), '\\n')\n",
    "        action_probs = policy_forward(params, batch)\n",
    "        print(np.round(action_probs.reshape(3,3), 2))\n",
    "        \n",
    "#         for action in range(dim_actions):\n",
    "#         print(obs_tensor_to_board(exp.s_next))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T02:19:43.430525Z",
     "start_time": "2021-05-02T02:19:43.416886Z"
    }
   },
   "outputs": [],
   "source": [
    "# @jit\n",
    "# def value_net_forward(value_params, x):\n",
    "#     return value_net.apply(value_params, x)\n",
    "\n",
    "# @jit\n",
    "# def value_net_error(value_params, batch):\n",
    "#     v = value_net_forward(value_params, batch['s'])\n",
    "#     target = batch['g']\n",
    "#     error = target - v\n",
    "#     return error\n",
    "\n",
    "# @jit\n",
    "# def value_net_loss(value_params, batch):\n",
    "#     error = value_net_error(value_params, batch)\n",
    "#     return jnp.mean(jnp.square(error))\n",
    "    \n",
    "# @jit\n",
    "# def value_update(value_params, value_opt_state, batch):\n",
    "#     loss, grads = value_and_grad(value_net_loss)(value_params, batch)\n",
    "#     updates, value_opt_state = value_opt.update(grads, value_opt_state)\n",
    "#     new_params = optax.apply_updates(value_params, updates)\n",
    "#     return loss, new_params, value_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T02:19:49.597134Z",
     "start_time": "2021-05-02T02:19:48.522573Z"
    }
   },
   "outputs": [],
   "source": [
    "num_games = 10000\n",
    "buffer = ReplayBuffer(capacity=50000)\n",
    "batch_size = 10000\n",
    "value_net_shape = (batch_size, 27)\n",
    "value_params = value_net.init(value_net_params_init_key, jnp.zeros(value_net_shape))\n",
    "value_opt = optax.adam(1e-5)\n",
    "value_opt_state = value_opt.init(value_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-02T02:19:23.362Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_games)):\n",
    "    exps = traj_to_exps()\n",
    "    for exp in exps:\n",
    "        buffer.add(exp)\n",
    "\n",
    "    if len(buffer) >= batch_size:\n",
    "        samples = buffer.sample(batch_size)\n",
    "        batch = create_batch(samples)\n",
    "        loss, value_params, value_opt_state = value_update(\n",
    "            value_params, value_opt_state, batch)\n",
    "        if i % 100 == 0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T02:17:26.220524Z",
     "start_time": "2021-05-02T02:17:26.165938Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exp in buffer.sample(20):\n",
    "    if exp:\n",
    "        batch = create_batch([exp])\n",
    "        print('g', exp.g)\n",
    "        print('v', value_net_forward(value_params, jnp.array(exp.s)))\n",
    "#         v_next = compute_v_next(value_params, batch)\n",
    "#         batch['v_next'] = v_next\n",
    "#         print('v_next', v_next)\n",
    "#         print('target', compute_target(batch))\n",
    "        print('loss', value_net_loss(value_params, batch))\n",
    "        print(obs_tensor_to_board(exp.s), '\\n')\n",
    "        print(obs_tensor_to_board(exp.s_next))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_playground_jax",
   "language": "python",
   "name": "rl_playground_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
